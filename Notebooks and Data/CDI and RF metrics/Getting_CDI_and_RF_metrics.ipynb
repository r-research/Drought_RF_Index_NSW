{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting CDI and reduced RF metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generated postcodes in google colab \n",
    "Then I extracted the postcode with excel formula - https://www.extendoffice.com/documents/excel/1660-excel-extract-postcode.html\n",
    "It extracted xxxx, Australia\n",
    "\n",
    "So I used Left value function to extract first 4 letters Source: https://www.exceldemy.com/extract-specific-numbers-from-a-cell-in-excel/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CDI files came in seperate files for each month. So I wanted to combine them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monthly CDI data by parish can be downloaded from here to complete the following section: https://edis.spaceport.intersect.org.au/%2FMonthly%20Data%2FEDISdata_byParish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do\n",
    "a) write a program that reads one file and writes a new with your added column (for example read a line, add your new column stringwise and write) \n",
    "\n",
    "b) take the part that reads and wrap that into a function that returns the lines to write. \n",
    "\n",
    "c) open your file that you want to write to\n",
    "\n",
    "d) make a loop over the files that you want to read, and then just write your new lines to the final file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_yr_mth_to_EDIS(name):\n",
    "\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    import datetime\n",
    "\n",
    "    name = name\n",
    "\n",
    "    # Extracting year and month from the name\n",
    "    # year = re.search(r'(\\d+)',name)\n",
    "    # OR\n",
    "    # using join() + isdigit() + filter()\n",
    "    # Extract digit string\n",
    "    year = ''.join(filter(lambda i: i.isdigit(), name)) #https://www.geeksforgeeks.org/python-extract-digits-from-given-string/\n",
    "    # Now remove the last 4 and only keep the first 4\n",
    "    year = year[:4] # Iterating through the first 4 numbers https://stackoverflow.com/questions/42788930/how-to-take-only-first-four-elements-of-every-strings-from-a-list\n",
    "    print(year)\n",
    "\n",
    "    # Doing the same for month\n",
    "    mth = ''.join(filter(lambda i: i.isdigit(), name))\n",
    "    # The second last digit and the fourth last digit\n",
    "    mth = mth[-4:-2]\n",
    "    \n",
    "    # Converting month from num to string:\n",
    "    #provide month number (Source:https://www.studytonight.com/python-howtos/how-to-get-month-name-from-month-number-in-python)\n",
    "    datetime_object = datetime.datetime.strptime(mth, \"%m\")\n",
    "    mth = datetime_object.strftime(\"%B\")\n",
    "    print(mth)\n",
    "\n",
    "\n",
    "    # Read file:\n",
    "    df_edis = pd.read_excel(name + '.xlsx')\n",
    "\n",
    "    # Solution 1: Adding column with same value\n",
    "    # yr_mth_dict = {'Year': 2016, 'Month': 'January'}\n",
    "    #df_edis.assign(**yr_mth_dict,inplace = True) \n",
    "\n",
    "    # Solution 2:\n",
    "    # adding column with constant value\n",
    "    df_edis['Year'] = pd.Series([year for x in range(len(df_edis.index))])\n",
    "    df_edis['Month'] = pd.Series([mth for x in range(len(df_edis.index))])\n",
    "    \n",
    "    df_edis.to_excel(str(name) + '_yr_mth.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Combining postcode reports\n",
    "import glob\n",
    "import os\n",
    "##Setting path of files and giving name\n",
    "files = os.path.join(\"edis_*.xlsx\")\n",
    "##List of merged files returned\n",
    "files = glob.glob(files)\n",
    "print(files)\n",
    "\n",
    "## Add year and month columns to files\n",
    "for file in files:\n",
    "   file_name = file.split('.') # Source: https://www.pythonpool.com/python-get-filename-without-extension/\n",
    "   print(file_name)\n",
    "   file_name = file_name[0]\n",
    "   print(file_name)\n",
    "   add_yr_mth_to_EDIS(file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moved all the old files back into a different folder\n",
    "Now I only have the new files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am going to combine all the EDIS files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Combining EDIS reports\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "##Setting path of files and giving name\n",
    "files = os.path.join(\"edis_*.xlsx\")\n",
    "##List of merged files returned\n",
    "files = glob.glob(files)\n",
    "files\n",
    "## Join files with concat\n",
    "df_files = pd.concat(map(pd.read_excel, files), ignore_index=True)\n",
    "# print(df_files)\n",
    "##Saving to csv\n",
    "df_files.to_csv('Combined_EDIS_2016_01_to_2021_12.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Opening database with postcodes 935 points\n",
    "db_postcode = pd.read_csv('Database_All_AWRA_NDVI_MOf_and_3MPrecip_noWel_postcode_data_2022_09_21.csv')\n",
    "#db_postcode.info()\n",
    "# Opening EDIS with postcodes\n",
    "df_files = pd.read_csv('Combined_EDIS_2016_01_to_2021_12.csv')\n",
    "# Capitalise Location of db_postcode OR make only the first letter capital and rest lower case in EDIS: Source:https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.capitalize.html\n",
    "df_files['parishname'] = df_files['parishname'].str.title() #If we just do the right side, it does not save it so we assign the same column back to the function we do to it to save it - example: https://datatofish.com/uppercase-pandas-dataframe/\n",
    "#df_files.info()\n",
    "\n",
    "# Renaming columns so can merge:\n",
    "df_files.rename(columns={'parishname':'Location','postcode':'postal_code'},inplace=True)\n",
    "#df_files.info()\n",
    "\n",
    "# Converting obj to string because when you merge on obj it can stuff up.\n",
    "db_postcode['postal_code'] = db_postcode['postal_code'].astype(str)\n",
    "df_files['postal_code'] = df_files['postal_code'].astype(str)\n",
    "\n",
    "# Merging:\n",
    "# https://www.statology.org/pandas-merge-multiple-columns/\n",
    "result = db_postcode.merge(df_files[['Location','CDI','Year','Month','postal_code']], on = ['postal_code','Year','Month'], how = 'left')\n",
    "# Dropping rows that dont meet below condition:\n",
    "result2 = result[result.Location_x == result.Location_y]\n",
    "result2.to_csv('Database_All_AWRA_NDVI_MOf_and_3MPrecip_noWel_postcode_MERGED_CDI_data_2022_09_22.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I made a database with CDI from the LGAs instead. I got much more. Now I will run the metrics on it. \n",
    "\n",
    "3 Oct 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5th October - redoing so that I can get lga and countyname for all locations. \n",
    "import pandas as pd\n",
    "# Opening database with postcodes 935 points\n",
    "db_postcode = pd.read_csv('Database_All_AWRA_NDVI_MOf_and_3MPrecip_noWel_postcode_data_2022_09_21.csv')\n",
    "#db_postcode.info()\n",
    "# Opening EDIS with postcodes\n",
    "df_files = pd.read_csv('Combined_EDIS_2016_01_to_2021_12.csv')\n",
    "# Capitalise Location of db_postcode OR make only the first letter capital and rest lower case in EDIS: Source:https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.capitalize.html\n",
    "df_files['parishname'] = df_files['parishname'].str.title() #If we just do the right side, it does not save it so we assign the same column back to the function we do to it to save it - example: https://datatofish.com/uppercase-pandas-dataframe/\n",
    "#df_files.info()\n",
    "\n",
    "# Renaming columns so can merge:\n",
    "df_files.rename(columns={'parishname':'Location','postcode':'postal_code'},inplace=True)\n",
    "#df_files.info()\n",
    "\n",
    "# Converting obj to string because when you merge on obj it can stuff up.\n",
    "db_postcode['postal_code'] = db_postcode['postal_code'].astype(str)\n",
    "df_files['postal_code'] = df_files['postal_code'].astype(str)\n",
    "\n",
    "# Merging:\n",
    "# https://www.statology.org/pandas-merge-multiple-columns/\n",
    "result = db_postcode.merge(df_files[['countyname','lganame','Location','CDI','Year','Month','postal_code']], on = ['postal_code','Year','Month'], how = 'left')\n",
    "# Dropping rows that dont meet below condition:\n",
    "result2 = result[result.Location_x == result.Location_y]\n",
    "result2.to_csv('Database_All_AWRA_NDVI_MOf_and_3MPrecip_noWel_postcode_MERGED_CDI_data_2022_10_05.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually sorted into Database_All_AWRA_NDVI_MOf_and_3MPrecip_noWel_postcode_MERGED_CDI_from_LGA_names_manual_sorting_data_2022_09_28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I also redid the parishname code and cleaned it checking with the arcgis map each coordinate and then I also combined the LGA one and the other one to get 702 lines. I removed any duplicates. So it is a slightly different dataset. It is only the 2018,2019,2020 and 2021 datapoints from the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_for_CDI(database_file_name, CDI_number):\n",
    "\n",
    "    ## IMPORTING:\n",
    "    # Importing numpy\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    # For plotting feature importance: \n",
    "    import matplotlib.pyplot as plt\n",
    "    import time\n",
    "    # For dividing training and testing  \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    #For training the model\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    # For metrics\n",
    "    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "    # For feature importance plots:\n",
    "    from sklearn.inspection import permutation_importance\n",
    "\n",
    "    # Assign variables:\n",
    "    database_file_name = database_file_name\n",
    "    CDI_number = CDI_number #This can be 7,6,5,4,3. USed for y_pred_0_test etc\n",
    "\n",
    "    # The name of the numbers included to define drought 0=drought(1), 0,1 = drought(1); 0,1,2=drought(1) and so on\n",
    "    CDI_thresholds = [0,1,2,'01234','0123','012','01','0'] #0 = -7 or CDI_number of 7\n",
    "    threshold_number = CDI_thresholds[CDI_number]\n",
    "\n",
    "    # Opening database with CDI values and CDI0,1234 values\n",
    "    db_CDI = pd.read_csv(database_file_name + '.csv') #Database_All_AWRA_NDVI_MOf_and_3MPrecip_noWel_postcode_MERGED_CDI_data_manual_check_2022_10_05.csv\n",
    "    db_CDI.drop(columns = ['Unnamed: 0.3','Unnamed: 0.2','Unnamed: 0','Unnamed: 0.1'],inplace=True)  \n",
    "    #This is specific to CDI file so make sure you have the correct one. \n",
    "\n",
    "    # Dividing data into predictors (X) and no drought/drought (y)\n",
    "    y = db_CDI.iloc[:,7].values #All rows and only the 7th column (Drought no drought)\n",
    "   \n",
    "    X = db_CDI \n",
    "    X.drop(['Drought / No Drought'],axis = 1,inplace=True)\n",
    "    #Convert to array\n",
    "    X.to_numpy()\n",
    "\n",
    "    # ----------------------------TRAINING AND RESULTS WITH DIFF RANDOM STATES---------------------------------------------------------------------------------------------------------------------------\n",
    "    # Creating a dictionary called list_metrics\n",
    "    acc_lm_CDI = []\n",
    "    tp_lm_CDI = []\n",
    "    tn_lm_CDI = []\n",
    "    fp_lm_CDI = []\n",
    "    fn_lm_CDI = []\n",
    "\n",
    "    tpr_lm = []\n",
    "    tnr_lm = []\n",
    "    ppv_lm = []\n",
    "    npv_lm = []\n",
    "    fpr_lm =[]\n",
    "    fnr_lm  = []\n",
    "    fdr_lm = []\n",
    "    acc_conf_lm = []\n",
    "\n",
    "    for i in range(51): \n",
    "        # ----------------------------SPLITTING TRAINING AND TESTING DATASET--------------------------------------------\n",
    "        #Dividing data into training and testing: \n",
    "        # from sklearn.model_selection import train_test_split\n",
    "        # I put it above\n",
    "    \n",
    "        X_train_all, X_test_all, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state= i)\n",
    "        \n",
    "        # Then we create the real X_train and X_test by removing those columns. This allows us to more easily see which ones were an issue. \n",
    "        X_train = X_train_all.drop(['Year_Month','Latitude','Longitude','Year','Month','Geographic Location','Location_x', 'MYD13A3_061__1_km_monthly_EVI','MYD13A3_061__1_km_monthly_NDVI','MYD13A3_061__1_km_monthly_VI_Quality_VI_Usefulness_Description','IPO_TPI'],axis = 1)\n",
    "        X_test = X_test_all.drop(['Year_Month','Latitude','Longitude','Year','Month','Geographic Location','Location_x','MYD13A3_061__1_km_monthly_EVI','MYD13A3_061__1_km_monthly_NDVI','MYD13A3_061__1_km_monthly_VI_Quality_VI_Usefulness_Description','IPO_TPI'],axis = 1) \n",
    "        \n",
    "        # To remove the last 10 rows that include CDI values:\n",
    "        # df.iloc[row_start:row_end , col_start:col_end] \n",
    "        # Source: https://thispointer.com/pandas-delete-last-column-of-dataframe-in-python/#:~:text=Use%20drop()%20to%20remove,last%20column%20of%20pandas%20dataframe.\n",
    "        X_train = X_train.iloc[:,0:10]\n",
    "        X_test =  X_test.iloc[:,0:10]\n",
    "        # Thus I took all rows and columns from 0 to 10 which means it deleted the last CDI rows. But I had to manually count them. \n",
    "        \n",
    "        # --------------------------------TRAINING THE MODEL---------------------------------------------        \n",
    "        # Training the model\n",
    "        # from sklearn.ensemble import RandomForestClassifier # See above\n",
    "        classifier = RandomForestClassifier(n_estimators=20, random_state=i)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        y_pred_classifier = classifier.predict(X_test)\n",
    "        \n",
    "        # ----------------------------- GETTING PREDICTIONS------------------------\n",
    "        #predictions for X_test\n",
    "        predictions = classifier.predict_proba(X_test)\n",
    "        # predictions # Col 1 is 0 and Col 2 is 1. We can\n",
    "        prob_drought = predictions[:,1]\n",
    "        # prob_drought\n",
    "        X_test_all['ypred'] = prob_drought\n",
    "        X_test_all['y_obs'] = y_test\n",
    "        #X_test_all.to_csv('X_test_all_'+ str(i) +'_5Oct.csv')\n",
    "\n",
    "        # predictions for X_train to make sure it is correct. This helps to check if it is classifying D/no D correctly\n",
    "        prediction_train = classifier.predict_proba(X_train)\n",
    "        # prediction_train # Col 1 is 0 and Col 2 is 1. We can\n",
    "        prob_drought_train = prediction_train[:,1]\n",
    "        \n",
    "        X_train_all['ypred'] = prob_drought_train\n",
    "        X_train_all['y_obs'] = y_train\n",
    "        #X_train_all.to_csv('X_train_all_'+ str(i) +'_5Oct.csv')\n",
    "        \n",
    "        # ----------------------------GETTING METRICS-------------------------------------------------------\n",
    "        # confusion matrix + other matrix\n",
    "        # from sklearn.metrics import classification_report, confusion_matrix, accuracy_score \n",
    "        # See above\n",
    "\n",
    "        # For confusion matrix later: https://datascience.stackexchange.com/questions/87654/is-there-a-flexible-way-to-get-the-original-data-indices-from-each-cell-of-a-con\n",
    "            \n",
    "        # False Positives:\n",
    "        # I got inspiration from this: https://datascience.stackexchange.com/questions/87654/is-there-a-flexible-way-to-get-the-original-data-indices-from-each-cell-of-a-con\n",
    "        #mistakes = np.invert(y_pred_classifier == y_test)\n",
    "        #count_fp = np.count_nonzero(mistakes)\n",
    "        #fp_list_metrics = np.append(fp_list_metrics,count_fp)\n",
    "        # I later found that it was just calculating mistakes and not false positive.\n",
    "\n",
    "        # FOR CDI\n",
    "        y_true_test = y_test #All rows and only the Drought/No drought col\n",
    "        y_pred_CDI_test = X_test_all.iloc[:,-CDI_number].values #0 = -5,0 and 1 = -4; 0 to 2 = -3; 0 to 3 = -2; 0 to 4= -1\n",
    "\n",
    "        #y_pred_0_test = X_test_all.iloc[:,-5].values\n",
    "        #y_pred_1_test = X_test_all.iloc[:,-4].values\n",
    "        #y_pred_2_test = X_test_all.iloc[:,-3].values\n",
    "        #y_pred_3_test = X_test_all.iloc[:,-2].values\n",
    "        #y_pred_4_test = X_test_all.iloc[:,-1].values\n",
    "\n",
    "        y_true_train = y_train #All rows and only the Drought/No drought col\n",
    "        y_pred_CDI_train = X_train_all.iloc[:,-CDI_number].values\n",
    "        #y_pred_0_train = X_train_all.iloc[:,-5].values\n",
    "        #y_pred_1_train = X_train_all.iloc[:,-4].values\n",
    "        #y_pred_2_train = X_train_all.iloc[:,-3].values\n",
    "        #y_pred_3_train = X_train_all.iloc[:,-2].values\n",
    "        #y_pred_4_train = X_train_all.iloc[:,-1].values\n",
    "        \n",
    "        # Confusion matrix for CDI - true neg, false pos, false neg, true pos:\n",
    "        tn_CDI, fp_CDI, fn_CDI, tp_CDI = confusion_matrix(y_true_test, y_pred_CDI_test, labels=[0, 1]).ravel()\n",
    "        \n",
    "        # I wanted to automatically calculate the rates as well. Source: https://stackoverflow.com/questions/50666091/true-positive-rate-and-false-positive-rate-tpr-fpr-for-multi-class-data-in-py\n",
    "        fp_CDI = fp_CDI.astype(float)\n",
    "        fn_CDI = fn_CDI.astype(float)\n",
    "        tp_CDI = tp_CDI.astype(float)\n",
    "        tn_CDI = tn_CDI.astype(float)\n",
    "\n",
    "        # Sensitivity, hit rate, recall, or true positive rate\n",
    "        TPR = tp_CDI/(tp_CDI+fn_CDI)\n",
    "        # Specificity or true negative rate\n",
    "        TNR = tn_CDI/(tn_CDI+fp_CDI) \n",
    "        # Precision or positive predictive value\n",
    "        PPV = tp_CDI/(tp_CDI+fp_CDI)\n",
    "        # Negative predictive value\n",
    "        NPV = tn_CDI/(tn_CDI+fn_CDI)\n",
    "        # Fall out or false positive rate\n",
    "        FPR = fp_CDI/(fp_CDI+tn_CDI)\n",
    "        # False negative rate\n",
    "        FNR = fn_CDI/(tp_CDI+fn_CDI)\n",
    "        # False discovery rate\n",
    "        FDR = fp_CDI/(tp_CDI+fp_CDI)\n",
    "        # Overall accuracy\n",
    "        ACC_conf = (tp_CDI+tn_CDI)/(tp_CDI+fp_CDI+fn_CDI+tn_CDI)\n",
    "\n",
    "        tn_lm_CDI = np.append(tn_lm_CDI,tn_CDI)\n",
    "        fp_lm_CDI = np.append(fp_lm_CDI,fp_CDI)\n",
    "        fn_lm_CDI = np.append(fn_lm_CDI,fn_CDI)\n",
    "        tp_lm_CDI = np.append(tp_lm_CDI,tp_CDI)\n",
    "        tpr_lm = np.append(tpr_lm, TPR)\n",
    "        tnr_lm = np.append(tnr_lm, TNR)\n",
    "        ppv_lm = np.append(ppv_lm, PPV)\n",
    "        npv_lm = np.append(npv_lm, NPV)\n",
    "        fpr_lm = np.append(fpr_lm, FPR)\n",
    "        fnr_lm  = np.append(fnr_lm, FNR)\n",
    "        fdr_lm = np.append(fdr_lm, FDR)\n",
    "        acc_conf_lm = np.append(acc_conf_lm, ACC_conf)\n",
    "\n",
    "        # Accuracy for CDI:\n",
    "        acc_CDI = accuracy_score(y_true_test, y_pred_CDI_test)\n",
    "        acc_lm_CDI= np.append(acc_lm_CDI,acc_CDI)\n",
    "\n",
    "        ## CDI Classification report:\n",
    "        # Source: https://stackoverflow.com/questions/39662398/scikit-learn-output-metrics-classification-report-into-csv-tab-delimited-format\n",
    "        # Function\n",
    "        def CDI_classification_report_csv(report,rd_state):\n",
    "            report_data = []\n",
    "            lines = report.split('\\n')\n",
    "            for line in lines[2:-5]:\n",
    "                row = {}\n",
    "                \n",
    "                row_data = ' '.join(line.split())   \n",
    "                row_data = row_data.split(' ')\n",
    "                \n",
    "                row['class'] = row_data[0]\n",
    "                row['precision'] = float(row_data[1])\n",
    "                row['recall'] = float(row_data[2])\n",
    "                row['f1_score'] = float(row_data[3])\n",
    "                row['support'] = float(row_data[4])\n",
    "                report_data.append(row)\n",
    "                # return report_data #https://stackoverflow.com/questions/3052793/python-output-from-functions\n",
    "\n",
    "            dataframe = pd.DataFrame.from_dict(report_data)\n",
    "            dataframe.to_csv('CDI_threshold'+ str(threshold_number) +'_classification_report_test_10Oct2022_'+ str(rd_state) + '.csv', index = False)\n",
    "        \n",
    "        report = classification_report(y_true_test, y_pred_CDI_test)\n",
    "        CDI_classification_report_csv(report,i) # Previously I assigned it a variable but it returns a csv anyway so I dont need to.\n",
    "\n",
    "        \n",
    "    # It is cheaper to fill out a list than fill out a dataframe: https://stackoverflow.com/questions/13784192/creating-an-empty-pandas-dataframe-then-filling-it\n",
    "    # For CDI:\n",
    "    df_acc_CDI = pd.DataFrame(acc_lm_CDI)\n",
    "    df_acc_CDI.columns = ['Accuracy_CDI_'+ str(threshold_number)]\n",
    "    df_acc_CDI.to_csv('CDI_'+ str(threshold_number) +'_Random_state_0_to_50_Acc_test_10Oct.csv')\n",
    "        \n",
    "    df_tn_CDI = pd.DataFrame(tn_lm_CDI)\n",
    "    df_tn_CDI.columns = ['True Negative_CDI_'+ str(threshold_number)]\n",
    "    df_tn_CDI.to_csv('CDI_'+ str(threshold_number) +'_Random_state_0_to_50_trueneg_test_10Oct.csv')\n",
    "\n",
    "    df_fp_CDI = pd.DataFrame(fp_lm_CDI)\n",
    "    df_fp_CDI.columns = ['False Positive_CDI_'+ str(threshold_number)]\n",
    "    df_fp_CDI.to_csv('CDI_'+ str(threshold_number) +'_Random_state_0_to_50_falsepos_test_10Oct.csv')\n",
    "\n",
    "    df_fn_CDI = pd.DataFrame(fn_lm_CDI)\n",
    "    df_fn_CDI.columns = ['False Negative_CDI_'+ str(threshold_number)]\n",
    "    df_fn_CDI.to_csv('CDI_'+ str(threshold_number) +'_Random_state_0_to_50_falseneg_test_10Oct.csv')\n",
    "\n",
    "    df_tp_CDI = pd.DataFrame(tp_lm_CDI)\n",
    "    df_tp_CDI.columns = ['True Positive_CDI_'+ str(threshold_number)]\n",
    "    df_tp_CDI.to_csv('CDI_'+ str(threshold_number) +'_Random_state_0_to_50_truepos_test_10Oct.csv')\n",
    "\n",
    "    tpr_lm_df = pd.DataFrame(tpr_lm)\n",
    "    tpr_lm_df.columns = ['True Positive Rate / Sensitivity_CDI_'+ str(threshold_number)]\n",
    "    tpr_lm_df.to_csv('CDI_'+ str(threshold_number) +'_Random_state_0_to_50_tpr_10Oct.csv')\n",
    "    tnr_lm_df = pd.DataFrame(tnr_lm)\n",
    "    tnr_lm_df.columns = ['True Negative Rate / Specificity_CDI_'+ str(threshold_number)]\n",
    "    tnr_lm_df.to_csv('CDI_'+ str(threshold_number) +'_Random_state_0_to_50_tnr_10Oct.csv')\n",
    "    ppv_lm_df = pd.DataFrame(ppv_lm)\n",
    "    ppv_lm_df.columns = ['Positive predictive value / Precision_CDI_'+ str(threshold_number)]\n",
    "    ppv_lm_df.to_csv('CDI_'+ str(threshold_number) +'_Random_state_0_to_50_ppv_10Oct.csv')\n",
    "    npv_lm_df = pd.DataFrame(npv_lm)\n",
    "    npv_lm_df.columns = ['Negative predictive value_CDI_'+ str(threshold_number)]\n",
    "    npv_lm_df.to_csv('CDI_'+ str(threshold_number) +'_Random_state_0_to_50_npv_10Oct.csv')\n",
    "    fpr_lm_df = pd.DataFrame(fpr_lm)\n",
    "    fpr_lm_df.columns = ['False Positive rate / Fall out_CDI_'+ str(threshold_number)]\n",
    "    fpr_lm_df.to_csv('CDI_'+ str(threshold_number) +'_Random_state_0_to_50_fpr_10Oct.csv')\n",
    "    fnr_lm_df = pd.DataFrame(fnr_lm)\n",
    "    fnr_lm_df.columns = ['False Negative rate_CDI_'+ str(threshold_number)]\n",
    "    fnr_lm_df.to_csv('CDI_'+ str(threshold_number) +'_Random_state_0_to_50_fnr_10Oct.csv')\n",
    "    fdr_lm_df = pd.DataFrame(fdr_lm)\n",
    "    fdr_lm_df.columns = ['False discovery rate_CDI_'+ str(threshold_number)]\n",
    "    fdr_lm_df.to_csv('CDI_'+ str(threshold_number) +'_Random_state_0_to_50_fdr_10Oct.csv')\n",
    "    acc_conf_lm_df = pd.DataFrame(acc_conf_lm)\n",
    "    acc_conf_lm_df.columns = ['Overall accuracy (from conf m)_CDI_'+ str(threshold_number)]\n",
    "    acc_conf_lm_df.to_csv('CDI_'+ str(threshold_number) +'_Random_state_0_to_50_acc_conf_10Oct.csv')\n",
    "\n",
    "    # CDI\n",
    "    ##Combining classification reports\n",
    "    import glob\n",
    "    import os\n",
    "    ##Setting path of files and giving name\n",
    "    files_CDI = os.path.join('CDI_threshold'+ str(threshold_number) +'_classification_report_test_10Oct2022_*.csv')\n",
    "    ##List of merged files returned\n",
    "    files_CDI = glob.glob(files_CDI)\n",
    "    ## Join files with concat\n",
    "    df_files_CDI = pd.concat(map(pd.read_csv, files_CDI), ignore_index=True)\n",
    "    # print(df_files)\n",
    "    ##Saving to csv\n",
    "    df_files_CDI.to_csv('Final_CDI_'+ str(threshold_number) +'_Random_state_0_to_50_Classification_Report_10Oct.csv')\n",
    "\n",
    "   \n",
    "    ## Combining Acc and Confusion Matrix: Source:https://www.statology.org/pandas-merge-multiple-dataframes/\n",
    "    from functools import reduce\n",
    "\n",
    "    #define list of DataFrames\n",
    "    dfs = [df_acc_CDI, df_fp_CDI, df_fn_CDI, df_tp_CDI , df_tn_CDI, tpr_lm_df,tnr_lm_df,ppv_lm_df,npv_lm_df,fpr_lm_df,fnr_lm_df,fdr_lm_df,acc_conf_lm_df]\n",
    "\n",
    "    #merge all DataFrames into one\n",
    "    #final_df = reduce(lambda  left,right: pd.merge(left,right,how='left'), dfs)\n",
    "    # Trying to merge on index singe the above merges it by attaching it below. \n",
    "    final_df = reduce(lambda  left,right: pd.merge(left,right,left_index = True,right_index=True, how='outer'), dfs)\n",
    "\n",
    "    final_df.to_csv('Combined_Metrics_Accuracy_Conf_Matrix_Rates_CDI_threshold'+ str(threshold_number) +'_0_to_50_test_10Oct.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The column number\n",
    "CDI_threshold_col_numbers = [7,6,5,4,3]\n",
    "\n",
    "database_CDI = 'Database_All_AWRA_NDVI_MOf_and_3MPrecip_noWel_postcode_MERGED_CDI_data_manual_check_2022_10_05'\n",
    "for CDI_threshold_col_number in CDI_threshold_col_numbers:\n",
    "    get_metrics_for_CDI(database_CDI,CDI_threshold_col_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting RF metrics for this database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_for_TEST_data(database,name_of_indice):\n",
    "\n",
    "    ## IMPORTING:\n",
    "    # Importing numpy\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    # For plotting feature importance: \n",
    "    import matplotlib.pyplot as plt\n",
    "    import time\n",
    "    # For dividing training and testing  \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    #For training the model\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    # For metrics\n",
    "    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "    # For feature importance plots:\n",
    "    from sklearn.inspection import permutation_importance\n",
    "\n",
    "    # Assign variables:\n",
    "    database = database\n",
    "    name_of_indice = name_of_indice\n",
    "    \n",
    "    # Dividing data into predictors (X) and no drought/drought (y)\n",
    "    y = database.iloc[:,7].values #All rows and only the 9th column (Drought no drought)\n",
    "    X = database \n",
    "    X.drop(['Drought / No Drought'],axis = 1,inplace=True)\n",
    "    #Convert to array\n",
    "    X.to_numpy()\n",
    "\n",
    "    # ----------------------------TRAINING AND RESULTS WITH DIFF RANDOM STATES---------------------------------------------------------------------------------------------------------------------------\n",
    "    # Creating a dictionary called list_metrics\n",
    "    acc_list_metrics = []\n",
    "    tp_list_metrics = []\n",
    "    tn_list_metrics = []\n",
    "    fp_list_metrics = []\n",
    "    fn_list_metrics = []\n",
    "\n",
    "    tpr_lm = []\n",
    "    tnr_lm = []\n",
    "    ppv_lm = []\n",
    "    npv_lm = []\n",
    "    fpr_lm = []\n",
    "    fnr_lm  = []\n",
    "    fdr_lm = []\n",
    "    acc_conf_lm = []\n",
    "\n",
    "\n",
    "    for i in range(51): \n",
    "        # ----------------------------SPLITTING TRAINING AND TESTING DATASET--------------------------------------------\n",
    "        #Dividing data into training and testing: \n",
    "        # from sklearn.model_selection import train_test_split\n",
    "        # I put it above\n",
    "    \n",
    "        X_train_all, X_test_all, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state= i)\n",
    "\n",
    "        # Then we create the real X_train and X_test by removing those columns. This allows us to more easily see which ones were an issue. \n",
    "        X_train = X_train_all.drop(['Year_Month','Latitude','Longitude','Year','Month','Geographic Location','Location_x','MYD13A3_061__1_km_monthly_EVI','MYD13A3_061__1_km_monthly_NDVI','MYD13A3_061__1_km_monthly_VI_Quality_VI_Usefulness_Description','IPO_TPI'],axis = 1)\n",
    "        X_test = X_test_all.drop(['Year_Month','Latitude','Longitude','Year','Month','Geographic Location','Location_x','MYD13A3_061__1_km_monthly_EVI','MYD13A3_061__1_km_monthly_NDVI','MYD13A3_061__1_km_monthly_VI_Quality_VI_Usefulness_Description','IPO_TPI'],axis = 1) \n",
    "        # To remove the last 10 rows that include SPI values:\n",
    "        # df.iloc[row_start:row_end , col_start:col_end] \n",
    "        # Source: https://thispointer.com/pandas-delete-last-column-of-dataframe-in-python/#:~:text=Use%20drop()%20to%20remove,last%20column%20of%20pandas%20dataframe.\n",
    "\n",
    "        X_train = X_train.iloc[:,0:10] \n",
    "        X_test =  X_test.iloc[:,0:10] \n",
    "        # Thus I took all rows and columns from 0 to 10 which means it deleted the last SPI rows. But I had to manually count them. \n",
    "\n",
    "        # --------------------------------TRAINING THE MODEL---------------------------------------------        \n",
    "        # Training the model\n",
    "        # from sklearn.ensemble import RandomForestClassifier # See above\n",
    "        classifier = RandomForestClassifier(n_estimators=20, random_state=i)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        y_pred_classifier = classifier.predict(X_test)\n",
    "        \n",
    "        # ----------------------------- GETTING PREDICTIONS------------------------\n",
    "        #predictions for X_test\n",
    "        predictions = classifier.predict_proba(X_test)\n",
    "        # predictions # Col 1 is 0 and Col 2 is 1. We can\n",
    "        prob_drought = predictions[:,1]\n",
    "        # prob_drought\n",
    "        X_test_all['ypred'] = prob_drought\n",
    "        X_test_all['y_obs'] = y_test\n",
    "        #X_test_all.to_csv('X_test_all_'+ str(i) +'_22Sept.csv')\n",
    "\n",
    "        # predictions for X_train to make sure it is correct. This helps to check if it is classifying D/no D correctly\n",
    "        prediction_train = classifier.predict_proba(X_train)\n",
    "        # prediction_train # Col 1 is 0 and Col 2 is 1. We can\n",
    "        prob_drought_train = prediction_train[:,1]\n",
    "        \n",
    "        X_train_all['ypred'] = prob_drought_train\n",
    "        X_train_all['y_obs'] = y_train\n",
    "        #X_train_all.to_csv('X_train_all_'+ str(i) +'_22Sept.csv')\n",
    "        \n",
    "        # ----------------------------GETTING METRICS-------------------------------------------------------\n",
    "        # confusion matrix + other matrix\n",
    "        # from sklearn.metrics import classification_report, confusion_matrix, accuracy_score \n",
    "        # See above\n",
    "\n",
    "        # For confusion matrix later: https://datascience.stackexchange.com/questions/87654/is-there-a-flexible-way-to-get-the-original-data-indices-from-each-cell-of-a-con\n",
    "            \n",
    "        # False Positives:\n",
    "        # I got inspiration from this: https://datascience.stackexchange.com/questions/87654/is-there-a-flexible-way-to-get-the-original-data-indices-from-each-cell-of-a-con\n",
    "        #mistakes = np.invert(y_pred_classifier == y_test)\n",
    "        #count_fp = np.count_nonzero(mistakes)\n",
    "        #fp_list_metrics = np.append(fp_list_metrics,count_fp)\n",
    "        # I later found that it was just calculating mistakes and not false positive.\n",
    "        \n",
    "\n",
    "        # Confusion matrix - true neg, false pos, false neg, true pos:\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred_classifier, labels=[0, 1]).ravel()\n",
    "\n",
    "        tn_list_metrics = np.append(tn_list_metrics,tn)\n",
    "        fp_list_metrics = np.append(fp_list_metrics,fp)\n",
    "        fn_list_metrics = np.append(fn_list_metrics,fn)\n",
    "        tp_list_metrics = np.append(tp_list_metrics,tp)\n",
    "\n",
    "        # RATES:\n",
    "        # Sensitivity, hit rate, recall, or true positive rate\n",
    "        TPR = tp/(tp+fn)\n",
    "        # Specificity or true negative rate\n",
    "        TNR = tn/(tn+fp) \n",
    "        # Precision or positive predictive value\n",
    "        PPV = tp/(tp+fp)\n",
    "        # Negative predictive value\n",
    "        NPV = tn/(tn+fn)\n",
    "        # Fall out or false positive rate\n",
    "        FPR = fp/(fp+tn)\n",
    "        # False negative rate\n",
    "        FNR = fn/(tp+fn)\n",
    "        # False discovery rate\n",
    "        FDR = fp/(tp+fp)\n",
    "        # Overall accuracy\n",
    "        ACC_conf = (tp+tn)/(tp+fp+fn+tn)\n",
    "\n",
    "        tpr_lm = np.append(tpr_lm, TPR)\n",
    "        tnr_lm = np.append(tnr_lm, TNR)\n",
    "        ppv_lm = np.append(ppv_lm, PPV)\n",
    "        npv_lm = np.append(npv_lm, NPV)\n",
    "        fpr_lm = np.append(fpr_lm, FPR)\n",
    "        fnr_lm  = np.append(fnr_lm, FNR)\n",
    "        fdr_lm = np.append(fdr_lm, FDR)\n",
    "        acc_conf_lm = np.append(acc_conf_lm, ACC_conf)\n",
    "\n",
    "        # Accuracy:\n",
    "        acc = accuracy_score(y_test, y_pred_classifier)\n",
    "        acc_list_metrics= np.append(acc_list_metrics,acc)\n",
    "\n",
    "\n",
    "        # Classification report:\n",
    "        # Source: https://stackoverflow.com/questions/39662398/scikit-learn-output-metrics-classification-report-into-csv-tab-delimited-format\n",
    "        # Function\n",
    "        def classification_report_csv(report,rd_state):\n",
    "            report_data = []\n",
    "            lines = report.split('\\n')\n",
    "            for line in lines[2:-5]:\n",
    "                row = {}\n",
    "                \n",
    "                row_data = ' '.join(line.split())   \n",
    "                row_data = row_data.split(' ')\n",
    "                \n",
    "                row['class'] = row_data[0]\n",
    "                row['precision'] = float(row_data[1])\n",
    "                row['recall'] = float(row_data[2])\n",
    "                row['f1_score'] = float(row_data[3])\n",
    "                row['support'] = float(row_data[4])\n",
    "                report_data.append(row)\n",
    "                # return report_data #https://stackoverflow.com/questions/3052793/python-output-from-functions\n",
    "\n",
    "            dataframe = pd.DataFrame.from_dict(report_data)\n",
    "            dataframe.to_csv('classification_report_9Oct2022_'+ str(rd_state) + 'RF_'+str(name_of_indice)+ '.csv', index = False)\n",
    "        \n",
    "        report = classification_report(y_test, y_pred_classifier)\n",
    "        classification_report_csv(report,i) # Previously I assigned it a variable but it returns a csv anyway so I dont need to. \n",
    "        \n",
    "    # It is cheaper to fill out a list than fill out a dataframe: https://stackoverflow.com/questions/13784192/creating-an-empty-pandas-dataframe-then-filling-it\n",
    "    df_acc_metrics = pd.DataFrame(acc_list_metrics)\n",
    "    df_acc_metrics.rename(columns = {\"0\":\"Accuracy\"}, inplace = True)\n",
    "    #df_acc_metrics.rename({'0':'Accuracy_RF_SPI'},)\n",
    "    df_acc_metrics.to_csv('Random_state_0_to_50_Acc_9Oct_RF'+str(name_of_indice)+'.csv')\n",
    "        \n",
    "    df_tn_metrics = pd.DataFrame(tn_list_metrics)\n",
    "    df_tn_metrics.rename(columns = {\"0\":\"True Negative\"}, inplace = True)\n",
    "    df_tn_metrics.to_csv('Random_state_0_to_50_trueneg_9Oct_RF'+str(name_of_indice)+'.csv')\n",
    "\n",
    "    df_fp_metrics = pd.DataFrame(fp_list_metrics)\n",
    "    df_fp_metrics.rename(columns = {\"0\":\"False Positive\"}, inplace = True)\n",
    "    df_fp_metrics.to_csv('Random_state_0_to_50_falsepos_9Oct_RF'+str(name_of_indice)+'.csv')\n",
    "\n",
    "    df_fn_metrics = pd.DataFrame(fn_list_metrics)\n",
    "    df_fn_metrics.rename(columns = {\"0\":\"False Negative\"}, inplace = True)\n",
    "    df_fn_metrics.to_csv('Random_state_0_to_50_falseneg_9Oct_RF'+str(name_of_indice)+'.csv')\n",
    "\n",
    "    df_tp_metrics = pd.DataFrame(tp_list_metrics)\n",
    "    df_tp_metrics.rename(columns = {\"0\":\"True Positive\"}, inplace = True)\n",
    "    df_tp_metrics.to_csv('Random_state_0_to_50_truepos_9Oct_RF'+str(name_of_indice)+'.csv')  \n",
    "    \n",
    "\n",
    "    tpr_lm_df = pd.DataFrame(tpr_lm)\n",
    "    tpr_lm_df.columns = ['True Positive Rate / Sensitivity_RF_'+str(name_of_indice)]\n",
    "    #tpr_lm_df.rename(columns = {\"0\":\"True Positive Rate / Sensitivity\"}, inplace = True)\n",
    "    tpr_lm_df.to_csv('Random_state_0_to_50_tpr_9Oct_RF'+str(name_of_indice)+'.csv')\n",
    "\n",
    "    tnr_lm_df = pd.DataFrame(tnr_lm)\n",
    "    tnr_lm_df.columns = ['True Negative Rate / Specificity_RF_'+str(name_of_indice)]\n",
    "    #tnr_lm_df.rename(columns = {\"0\":\"True Negative Rate / Specificity\"}, inplace = True)\n",
    "    tnr_lm_df.to_csv('Random_state_0_to_50_tnr_9Oct_RF'+str(name_of_indice)+'.csv')\n",
    "    \n",
    "    ppv_lm_df = pd.DataFrame(ppv_lm)\n",
    "    ppv_lm_df.columns = ['Positive predictive value / Precision_RF_'+str(name_of_indice)]\n",
    "    #ppv_lm_df.rename(columns = {\"0\":\"Positive predictive value / Precision\"}, inplace = True)\n",
    "    ppv_lm_df.to_csv('Random_state_0_to_50_ppv_9Oct_RF'+str(name_of_indice)+'.csv')\n",
    "    \n",
    "    npv_lm_df = pd.DataFrame(npv_lm)\n",
    "    npv_lm_df.columns = ['Negative predictive value_RF_'+str(name_of_indice)]\n",
    "    #npv_lm_df.rename(columns = {\"0\":\"Negative predictive value\"}, inplace = True)\n",
    "    npv_lm_df.to_csv('Random_state_0_to_50_npv_9Oct_RF'+str(name_of_indice)+'.csv')\n",
    "\n",
    "    fpr_lm_df = pd.DataFrame(fpr_lm)\n",
    "    fpr_lm_df.columns = ['False Positive rate / Fall out_RF_'+str(name_of_indice)]\n",
    "    #fpr_lm_df.rename(columns = {\"0\":\"False Positive rate / Fall out\"}, inplace = True)\n",
    "    fpr_lm_df.to_csv('Random_state_0_to_50_fpr_9Oct_RF'+str(name_of_indice)+'.csv')\n",
    "\n",
    "    fnr_lm_df = pd.DataFrame(fnr_lm)\n",
    "    fnr_lm_df.columns = ['False Negative rate_RF_'+str(name_of_indice)]\n",
    "    #fnr_lm_df.rename(columns = {\"0\":\"False Negative rate\"}, inplace = True)\n",
    "    fnr_lm_df.to_csv('Random_state_0_to_50_fnr_9Oct_RF'+str(name_of_indice)+'.csv')\n",
    "\n",
    "    fdr_lm_df = pd.DataFrame(fdr_lm)\n",
    "    fdr_lm_df.columns = ['False discovery rate_RF_'+str(name_of_indice)]\n",
    "    #fdr_lm_df.rename(columns = {\"0\":\"False discovery rate\"}, inplace = True)\n",
    "    fdr_lm_df.to_csv('Random_state_0_to_50_fdr_9Oct_RF'+str(name_of_indice)+'.csv')\n",
    "\n",
    "    acc_conf_lm_df = pd.DataFrame(acc_conf_lm)\n",
    "    acc_conf_lm_df.columns = ['Overall accuracy (from conf m)_RF_'+str(name_of_indice)]\n",
    "    #acc_conf_lm_df.rename(columns = {\"0\":\"Overall accuracy (from conf m)\"}, inplace = True)\n",
    "    acc_conf_lm_df.to_csv('Random_state_0_to_50_acc_conf_9Oct_RF'+str(name_of_indice)+'.csv')\n",
    "\n",
    "\n",
    "    ##Combining classification reports\n",
    "    import glob\n",
    "    import os\n",
    "    ##Setting path of files and giving name\n",
    "    files = os.path.join(\"classification_report_9Oct2022*.csv\")\n",
    "    ##List of merged files returned\n",
    "    files = glob.glob(files)\n",
    "    ## Join files with concat\n",
    "    df_files = pd.concat(map(pd.read_csv, files), ignore_index=True)\n",
    "    # print(df_files)\n",
    "    ##Saving to csv\n",
    "    df_files.to_csv('Random_state_0_to_50_Classification_Report_Final_9Oct_RF_' + str(name_of_indice)+'.csv')\n",
    "\n",
    "\n",
    "    # # To just combine accuracy and confusion matrix    \n",
    "    ## Combining Acc and Confusion Matrix: Source:https://www.statology.org/pandas-merge-multiple-dataframes/\n",
    "    from functools import reduce\n",
    "\n",
    "    #define list of DataFrames\n",
    "    dfs = [df_acc_metrics, df_fp_metrics, df_fn_metrics, df_tp_metrics , df_tn_metrics, tpr_lm_df,tnr_lm_df,ppv_lm_df,npv_lm_df,fpr_lm_df,fnr_lm_df,fdr_lm_df,acc_conf_lm_df]\n",
    "\n",
    "    #merge all DataFrames into one\n",
    "    #final_df = reduce(lambda  left,right: pd.merge(left,right,how='left'), dfs)\n",
    "    # Trying to merge on index singe the above merges it by attaching it below. \n",
    "    final_df = reduce(lambda left,right: pd.merge(left,right,left_index = True,right_index=True, how='outer'), dfs)\n",
    "\n",
    "    final_df.to_csv('Combined_Metrics_Accuracy_Conf_Matrix_X_test_for_RF_for_'+str(name_of_indice)+'_random_state_0_to_50_9Oct.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening database with indice values\n",
    "db = pd.read_csv('Database_All_AWRA_NDVI_MOf_and_3MPrecip_noWel_postcode_MERGED_CDI_data_manual_check_2022_10_05.csv') \n",
    "db.head()\n",
    "# This below is specific for SPI\n",
    "db.drop(columns = ['Unnamed: 0.3','Unnamed: 0.2','Unnamed: 0','Unnamed: 0.1'],inplace=True)\n",
    "get_metrics_for_TEST_data(db,'CDI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining CDI and RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_metrics = pd.read_csv('Combined_Metrics_Accuracy_Conf_Matrix_X_test_for_RF_for_CDI_random_state_0_to_50_9Oct.csv')\n",
    "CDI_metrics = pd.read_csv('All_Combined_Metrics_Accuracy_Conf_Matrix_Rates_CDI_all_thresholds_Final_10Oct.csv')\n",
    "\n",
    "# # To just combine    \n",
    "## Source:https://www.statology.org/pandas-merge-multiple-dataframes/\n",
    "from functools import reduce\n",
    "\n",
    "#define list of DataFrames\n",
    "dfs = [RF_metrics,CDI_metrics]\n",
    "\n",
    "# Trying to merge on index singe the above merges it by attaching it below. \n",
    "final_df = reduce(lambda left,right: pd.merge(left,right,left_index = True,right_index=True, how='outer'), dfs)\n",
    "\n",
    "final_df.to_csv('Combined_Metrics_Accuracy_Conf_Matrix_for_CDI_and_RF_random_state_0_to_50_10Oct.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redoing the plots without Recall and Precision\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Looping through \n",
    "# Making a tuple that has the values in the column name we want to remove\n",
    "column_names = ['CDI_0','CDI_01','CDI_012','CDI_0123','CDI_01234'] #This is a tuple. startswith only takes a tuple of strings\n",
    "# __contains only takes one str\n",
    "for cdi_name_value in column_names:\n",
    "    # Reading in file and removing unnamed columns and overall accuracy\n",
    "    cdi_rf_metrics = pd.read_csv('Combined_Metrics_Accuracy_Conf_Matrix_for_CDI_and_RF_random_state_0_to_50_10Oct.csv')\n",
    "    # Getting a list of column names:\n",
    "    metrics_col_name_list = cdi_rf_metrics.columns.tolist()\n",
    "    # Making a tuple that has the start of the column name we want to remove\n",
    "    unnamed_ov_acc_list = ('Unnamed:','Overall accuracy') #This is a tuple. startswith only takes a tuple of strings\n",
    "    #metrics_col_name_list\n",
    "    # creating a list of all the col names that start with Unnamed\n",
    "    all_unnamed = [x for x in metrics_col_name_list if x.startswith(unnamed_ov_acc_list)]\n",
    "    #print(all_unnamed)\n",
    "    cdi_rf_metrics.drop(columns=all_unnamed,inplace=True)\n",
    "    # Now removing all of these\n",
    "    metrics_col_name_list = cdi_rf_metrics.columns.tolist()\n",
    "    tn_fn_tp_fp_names = ('True Positive_CDI','False Positive_CDI','True Negative_CDI','False Negative_CDI','True Positive_RF','False Positive_RF','True Negative_RF','False Negative_RF','True Positive Rate','Positive predictive value')\n",
    "    # creating a list of all the col names that start with Unnamed\n",
    "    all_contains = [x for x in metrics_col_name_list if x.startswith(tn_fn_tp_fp_names)]\n",
    "    cdi_rf_metrics.drop(columns=all_contains,inplace=True)\n",
    "\n",
    "    # Getting a list of column names in dataframe:\n",
    "    metrics_col_name_list = cdi_rf_metrics.columns.tolist()\n",
    "\n",
    "    # Creating tuple of SPI names in dataframe and removing the SPI value I want to plot\n",
    "    c_names = ['CDI_0','CDI_01','CDI_012','CDI_0123','CDI_01234']  #This is a tuple. startswith only takes a tuple of strings\n",
    "    c_names.remove(cdi_name_value) #https://www.studytonight.com/post/python-how-to-remove-a-specific-element-from-a-list-of-tuples\n",
    "    print(c_names)\n",
    "\n",
    "    for column_n in c_names:\n",
    "        # creating a list of all the col names that contain the above\n",
    "        all_contains = [x for x in metrics_col_name_list if x.endswith(column_n)]\n",
    "        cdi_rf_metrics.drop(columns=all_contains,inplace=True)\n",
    "        \n",
    "    # Assigning it to a new db name\n",
    "    rf_cdi_db = cdi_rf_metrics\n",
    "\n",
    "    # Rearranging columns alphabetically\n",
    "    # https://iqcode.com/code/python/how-to-sort-columns-alphabetically-in-pandas\n",
    "    # Creating a list of column names\n",
    "    col_name = list(rf_cdi_db.columns)\n",
    "    print(col_name)\n",
    "    # Sorting alphabetically\n",
    "    col_name.sort()\n",
    "    \n",
    "    # Sorting columnas and reassigning to the dataframe\n",
    "    rf_cdi_db = rf_cdi_db[col_name]\n",
    "    # Renaming columns so the labels in the figures are shorter and only the title says what CDI it is\n",
    "    # Example of what one is: ['Accuracy_CDI_0', 'Accuracy_RF', 'False Negative rate_CDI_0', 'False Negative rate_RF_CDI', 'False Positive rate / Fall out_CDI_0', 'False Positive rate / Fall out_RF_CDI', 'False discovery rate_CDI_0', 'False discovery rate_RF_CDI', 'Negative predictive value_CDI_0', 'Negative predictive value_RF_CDI', 'Positive predictive value / Precision_CDI_0', 'Positive predictive value / Precision_RF_CDI', 'True Negative Rate / Specificity_CDI_0', 'True Negative Rate / Specificity_RF_CDI', 'True Positive Rate / Sensitivity_CDI_0', 'True Positive Rate / Sensitivity_RF_CDI']\n",
    "    rf_cdi_db.columns = ['Accuracy (CDI)', 'Accuracy (RF)', 'FN rate (CDI)', 'FN Rate (RF)', 'FP rate (CDI)', 'FP rate (RF)', 'FDR (CDI)', 'FDR (RF)', 'NPV (CDI)', 'NPV (RF)', 'TN Rate (CDI)', 'TN Rate (RF)'] # https://fedingo.com/how-to-rename-columns-in-pandas/\n",
    "   \n",
    "    # Creating figure\n",
    "    sns.set(style='whitegrid')\n",
    "    #facecolor = '#eaeaf2'\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))#, facecolor=facecolor)\n",
    "\n",
    "    # Specifying color of boxplot RF and CDI\n",
    "    #create your own color array\n",
    "    my_colors =['#F5793A','#3498db']#[\"#ffa500\", \"#3498db\"] #[\"#2ecc71\", \"#006a4e\"]\n",
    "    # Useful links: https://www.geeksforgeeks.org/how-to-use-seaborn-color-palette-to-color-boxplot/\n",
    "    # If the color palette has fewer colours then it repeats themselves. This allows every two to be differet: https://www.geeksforgeeks.org/python-plotly-how-to-set-up-a-color-palette/ \n",
    "    \n",
    "    # Create box plot:\n",
    "    ax = sns.boxplot(data=rf_cdi_db, \n",
    "                    palette= my_colors, \n",
    "                    linewidth=1.2, \n",
    "                    fliersize=2,\n",
    "                    flierprops=dict(marker='o', markersize=4)) \n",
    "\n",
    "    # Getting a list of all column names:\n",
    "    Col_names = list(rf_cdi_db.columns.values)\n",
    "    print(Col_names)\n",
    "\n",
    "    # Rotating the x labels - Source: https://www.geeksforgeeks.org/rotate-axis-tick-labels-in-seaborn-and-matplotlib/\n",
    "    ax.set_xticklabels(labels=Col_names, rotation=90) \n",
    "    \n",
    "    # Set labels font parameter\n",
    "    font_color = '#525252'\n",
    "    csfont = {'fontname':'Georgia'}\n",
    "    hfont = {'fontname':'Calibri'}\n",
    "\n",
    "    ax.set_ylabel('Score', fontsize=16, color=font_color, **hfont)\n",
    "    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        label.set(fontsize=16, color=font_color, **hfont)\n",
    "\n",
    "    # Setting the range of the y axis \n",
    "    ax.set(ylim=(-0.01, 1.01)) #https://stackoverflow.com/questions/33227473/how-to-set-the-range-of-y-axis-for-a-seaborn-boxplot\n",
    "\n",
    "    # Create title:\n",
    "    title = cdi_name_value\n",
    "    fig.suptitle(title, y=.97, fontsize=22, color=font_color, **csfont)\n",
    "    #subtitle = 'Random state: 0 to 50'\n",
    "    #plt.title(subtitle, fontsize=18, pad=10, color=font_color, **hfont)\n",
    "    plt.subplots_adjust(top=0.85)\n",
    "\n",
    "    # Set colour of outlier points: \n",
    "    for i, box in enumerate(ax.artists):\n",
    "        col = box.get_facecolor()\n",
    "        plt.setp(ax.lines[i*6+5], mfc=col, mec=col)\n",
    "\n",
    "\n",
    "    # But there are values at 0 which are not 0. I want to have more than 0.0 but 3 dec places. \n",
    "    # Potential other way: https://pyquestions.com/labeling-boxplot-in-seaborn-with-median-value \n",
    "    # Interesting plotting: https://stackoverflow.com/questions/60995546/box-plot-with-divisor-in-seaborn-python\n",
    "    # Will do it later.\n",
    "\n",
    "    # Adding legend:\n",
    "    #handles, _ = ax.get_legend_handles_labels()\n",
    "    #ax.legend(handles,['CDI','RF'],loc = 'best') # https://stackoverflow.com/questions/62069967/adding-a-legend-to-a-boxplot-in-matplotlib-seaborn\n",
    "    custom_lines = [Line2D([0], [0], color='#F5793A', lw=4),\n",
    "                    Line2D([0], [0], color='#3498db', lw=4)]\n",
    "                    \n",
    "    ax.legend(custom_lines, ['CDI', 'RF']) # Source: https://matplotlib.org/stable/gallery/text_labels_and_annotations/custom_legends.html\n",
    "\n",
    "    # Making sure that the x labels all fit in the saved figure: https://stackoverflow.com/questions/45239261/matplotlib-savefig-text-chopped-off\n",
    "    # Saving the figure:\n",
    "    filename = 'sns-boxplot_RFvs'+ str(cdi_name_value)\n",
    "    plt.savefig(filename+'7_Nov_22.png',bbox_inches = \"tight\",dpi = 300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting recall, precision and F1 score plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Looping through \n",
    "# Making a tuple that has the values in the column name we want to remove\n",
    "column_names = ['CDI_0','CDI_01','CDI_012','CDI_0123','CDI_01234'] #This is a tuple. startswith only takes a tuple of strings\n",
    "# __contains only takes one str\n",
    "\n",
    "# Reading in file and removing unnamed columns and overall accuracy\n",
    "cdi_rf_metrics = pd.read_csv('Precision_Recall_F1_score_combined_RF_and_CDI_2_Nov_22.csv')\n",
    "\n",
    "# Dropping all class 0 and keeping class 1: (https://www.golinuxcloud.com/pandas-drop-rows-examples/)\n",
    "cdi_rf_metrics = cdi_rf_metrics.drop(cdi_rf_metrics[cdi_rf_metrics['class'] == 0].index)\n",
    "# Getting a list of column names:\n",
    "metrics_col_name_list = cdi_rf_metrics.columns.tolist()\n",
    "# Making a tuple that has the start of the column name we want to remove\n",
    "unnamed_ov_acc_list = ('Unnamed: 0','support','class') #This is a tuple. startswith only takes a tuple of strings\n",
    "#metrics_col_name_list\n",
    "# creating a list of all the col names that start with Unnamed\n",
    "all_unnamed = [x for x in metrics_col_name_list if x.startswith(unnamed_ov_acc_list)]\n",
    "#print(all_unnamed)\n",
    "cdi_rf_metrics.drop(columns=all_unnamed,inplace=True)\n",
    "    \n",
    "# Assigning it to a new db name\n",
    "rf_cdi_db = cdi_rf_metrics\n",
    "\n",
    "# Rearranging columns alphabetically\n",
    "# https://iqcode.com/code/python/how-to-sort-columns-alphabetically-in-pandas\n",
    "# Creating a list of column names\n",
    "col_name = list(rf_cdi_db.columns)\n",
    "print(col_name)\n",
    "# Sorting alphabetically\n",
    "col_name.sort()\n",
    "\n",
    "# Sorting columnas and reassigning to the dataframe\n",
    "rf_cdi_db = rf_cdi_db[col_name]\n",
    "# Renaming columns so the labels in the figures are shorter and only the title says what CDI it is\n",
    "# Example of what one is: [[['f1_score_CDI0', 'f1_score_CDI_01', 'f1_score_CDI_012', 'f1_score_CDI_0123', 'f1_score_CDI_01234', 'f1_score_RF', 'precision_CDI0', 'precision_CDI_01', 'precision_CDI_012', 'precision_CDI_0123', 'precision_CDI_01234', 'precision_RF', 'recall_CDI0', 'recall_CDI_01', 'recall_CDI_012', 'recall_CDI_0123', 'recall_CDI_01234', 'recall_RF']]\n",
    "rf_cdi_db.columns =['F1 score (CDI_0)', 'F1 score (CDI_01)', 'F1 score (CDI_012)', 'F1 score (CDI_0123)', 'F1 score (CDI_01234)', 'F1 score (RF)', 'Precision (CDI_0)', 'Precision (CDI_01)', 'Precision (CDI_012)',\n",
    "                    'Precision (CDI_0123)', 'Precision (CDI_01234)', 'Precision (RF)', 'Recall (CDI_0)', 'Recall (CDI_01)', 'Recall (CDI_012)', 'Recall (CDI_0123)', 'Recall (CDI_01234)', 'Recall (RF)']\n",
    "#rf_cdi_db.columns =['F1 score', 'F1 score', 'F1 score', 'F1 score', 'F1 score', 'F1 score', 'Precision', 'Precision', 'Precision', 'Precision', 'Precision', 'Precision', 'Recall', 'Recall', 'Recall', 'Recall', 'Recall', 'Recall']\n",
    "\n",
    "# Creating figure\n",
    "sns.set(style='whitegrid')\n",
    "#facecolor = '#eaeaf2'\n",
    "fig, ax = plt.subplots(figsize=(12, 7))#, facecolor=facecolor)\n",
    "\n",
    "# Specifying color of boxplot RF and CDI\n",
    "#create your own color array\n",
    "#my_colors = ['#984ea3', '#ff7f00', '#4daf4a',\n",
    "#                  '#f781bf', '#BDB8AD', '#377eb8'] #Palette from https://gist.github.com/thriveth/8560036 - replaced a65628 with BDB8AD\n",
    "my_colors =['#BDB8AD','#F5793A','#A95AA1','#4daf4a',\n",
    "                  '#f781bf','#377eb8']\n",
    "# Useful links: https://www.geeksforgeeks.org/how-to-use-seaborn-color-palette-to-color-boxplot/\n",
    "# If the color palette has fewer colours then it repeats themselves. This allows every two to be differet: https://www.geeksforgeeks.org/python-plotly-how-to-set-up-a-color-palette/ \n",
    "\n",
    "# Create box plot:\n",
    "ax = sns.boxplot(data=rf_cdi_db, \n",
    "                palette= my_colors, \n",
    "                linewidth=1.2, \n",
    "                fliersize=2,\n",
    "                flierprops=dict(marker='o', markersize=4)) \n",
    "\n",
    "# Getting a list of all column names:\n",
    "Col_names = list(rf_cdi_db.columns.values)\n",
    "print(Col_names)\n",
    "\n",
    "# Rotating the x labels - Source: https://www.geeksforgeeks.org/rotate-axis-tick-labels-in-seaborn-and-matplotlib/\n",
    "ax.set_xticklabels(labels=Col_names, rotation=90) \n",
    "\n",
    "# Set labels font parameter\n",
    "font_color = '#525252'\n",
    "csfont = {'fontname':'Georgia'}\n",
    "hfont = {'fontname':'Calibri'}\n",
    "\n",
    "ax.set_ylabel('Score', fontsize=16, color=font_color, **hfont)\n",
    "for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    label.set(fontsize=16, color=font_color, **hfont)\n",
    "\n",
    "# Setting the range of the y axis \n",
    "ax.set(ylim=(-0.01, 1.01)) #https://stackoverflow.com/questions/33227473/how-to-set-the-range-of-y-axis-for-a-seaborn-boxplot\n",
    "\n",
    "# Create title:\n",
    "title = 'CDI and RF'\n",
    "fig.suptitle(title, y=.97, fontsize=22, color=font_color, **csfont)\n",
    "#subtitle = 'Random state: 0 to 50'\n",
    "#plt.title(subtitle, fontsize=18, pad=10, color=font_color, **hfont)\n",
    "plt.subplots_adjust(top=0.85)\n",
    "\n",
    "# Set colour of outlier points: \n",
    "for i, box in enumerate(ax.artists):\n",
    "    col = box.get_facecolor()\n",
    "    plt.setp(ax.lines[i*6+5], mfc=col, mec=col)\n",
    "\n",
    "\n",
    "# But there are values at 0 which are not 0. I want to have more than 0.0 but 3 dec places. \n",
    "# Potential other way: https://pyquestions.com/labeling-boxplot-in-seaborn-with-median-value \n",
    "# Interesting plotting: https://stackoverflow.com/questions/60995546/box-plot-with-divisor-in-seaborn-python\n",
    "# Will do it later.\n",
    "\n",
    "# Adding legend:\n",
    "#handles, _ = ax.get_legend_handles_labels()\n",
    "#ax.legend(handles,['CDI','RF'],loc = 'best') # https://stackoverflow.com/questions/62069967/adding-a-legend-to-a-boxplot-in-matplotlib-seaborn\n",
    "custom_lines = [Line2D([0], [0], color='#BDB8AD', lw=4),\n",
    "                 Line2D([0], [0], color='#F5793A', lw=4),\n",
    "                 Line2D([0], [0], color='#A95AA1', lw=4),\n",
    "                 Line2D([0], [0], color='#4daf4a', lw=4),\n",
    "                 Line2D([0], [0], color='#f781bf', lw=4),\n",
    "                 Line2D([0], [0], color='#377eb8', lw=4)]\n",
    "                \n",
    "ax.legend(custom_lines, ['CDI_0','CDI_01','CDI_012','CDI_0123','CDI_01234', 'RF']) # Source: https://matplotlib.org/stable/gallery/text_labels_and_annotations/custom_legends.html\n",
    "\n",
    "# Making sure that the x labels all fit in the saved figure: https://stackoverflow.com/questions/45239261/matplotlib-savefig-text-chopped-off\n",
    "# Saving the figure:\n",
    "filename = 'sns-boxplot_RFvsCDI_f1_score_P_R'\n",
    "plt.savefig(filename+'7_Nov_22.png',bbox_inches = \"tight\",dpi=300)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('Honours_2022')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "07839a5161a50ad95f9c2315d909b8e44b3141537b3172f602e0768d5d17d657"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
